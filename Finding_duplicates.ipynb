{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e737afdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: docx2txt in c:\\users\\817840\\anaconda3\\lib\\site-packages (0.8)\n",
      "Requirement already satisfied: sentence_transformers in c:\\users\\817840\\anaconda3\\lib\\site-packages (2.6.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\817840\\appdata\\roaming\\python\\python39\\site-packages (from sentence_transformers) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\817840\\appdata\\roaming\\python\\python39\\site-packages (from sentence_transformers) (1.10.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\817840\\appdata\\roaming\\python\\python39\\site-packages (from sentence_transformers) (2.2.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.32.0 in c:\\users\\817840\\appdata\\roaming\\python\\python39\\site-packages (from sentence_transformers) (4.37.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\817840\\appdata\\roaming\\python\\python39\\site-packages (from sentence_transformers) (4.66.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\817840\\anaconda3\\lib\\site-packages (from sentence_transformers) (1.0.2)\n",
      "Requirement already satisfied: Pillow in c:\\users\\817840\\appdata\\roaming\\python\\python39\\site-packages (from sentence_transformers) (10.2.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in c:\\users\\817840\\appdata\\roaming\\python\\python39\\site-packages (from sentence_transformers) (0.20.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\817840\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2024.2.0)\n",
      "Requirement already satisfied: requests in c:\\users\\817840\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\817840\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (4.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\817840\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\817840\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (6.0.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\817840\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (3.13.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\817840\\appdata\\roaming\\python\\python39\\site-packages (from torch>=1.11.0->sentence_transformers) (3.1.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\817840\\appdata\\roaming\\python\\python39\\site-packages (from torch>=1.11.0->sentence_transformers) (3.2.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\817840\\appdata\\roaming\\python\\python39\\site-packages (from torch>=1.11.0->sentence_transformers) (1.12)\n",
      "Requirement already satisfied: colorama in c:\\users\\817840\\anaconda3\\lib\\site-packages (from tqdm->sentence_transformers) (0.4.5)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\817840\\appdata\\roaming\\python\\python39\\site-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers) (0.3.2)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\817840\\appdata\\roaming\\python\\python39\\site-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers) (0.15.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\817840\\appdata\\roaming\\python\\python39\\site-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers) (2023.12.25)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\817840\\anaconda3\\lib\\site-packages (from scikit-learn->sentence_transformers) (2.2.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\817840\\appdata\\roaming\\python\\python39\\site-packages (from scikit-learn->sentence_transformers) (1.3.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\817840\\appdata\\roaming\\python\\python39\\site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\817840\\appdata\\roaming\\python\\python39\\site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\817840\\appdata\\roaming\\python\\python39\\site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\817840\\appdata\\roaming\\python\\python39\\site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2024.2.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\817840\\appdata\\roaming\\python\\python39\\site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.3.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\817840\\appdata\\roaming\\python\\python39\\site-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
      "Requirement already satisfied: PyPDF2 in c:\\users\\817840\\appdata\\roaming\\python\\python39\\site-packages (3.0.1)\n",
      "Requirement already satisfied: typing_extensions>=3.10.0.0 in c:\\users\\817840\\appdata\\roaming\\python\\python39\\site-packages (from PyPDF2) (4.9.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install docx2txt\n",
    "!pip install sentence_transformers\n",
    "!pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae93cbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import hashlib\n",
    "from shutil import move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4034532",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import PyPDF2\n",
    "import docx2txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3656f226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define directory paths\n",
    "document_folder = \"C:\\\\Users\\\\817840\\\\OneDrive - Cognizant\\\\2024\\\\AI-Engineer-Roadmap-2024\\\\Project POC\\\\Finding duplicates and % match\\\\Files\"\n",
    "duplicates_folder = \"C:\\\\Users\\\\817840\\\\OneDrive - Cognizant\\\\2024\\\\AI-Engineer-Roadmap-2024\\\\Project POC\\\\Finding duplicates and % match\\\\Files\\\\Duplicate_files\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94e18654",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hash_database(folder_path):\n",
    "    hash_database = {} #creating a dictionary for hash_database\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        if os.path.isfile(file_path):\n",
    "            try:\n",
    "                with open(file_path, 'rb') as f:\n",
    "                    md5_hash = hashlib.md5(f.read()).hexdigest()\n",
    "                    sha256_hash = hashlib.sha256(f.read()).hexdigest()\n",
    "                hash_database[filename] = (md5_hash, sha256_hash)\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Error: File '{filename}' not found. Hence skipping...\")\n",
    "    return hash_database   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4df6bc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_duplicate(filename, hash_database):\n",
    "    if filename in hash_database:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bab1ceb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_duplicate(filename, source_folder, destination_folder):\n",
    "    source_path = os.path.join(source_folder, filename)\n",
    "    destination_path = os.path.join(destination_folder, filename)\n",
    "    try:\n",
    "        move(source_path, destination_path)\n",
    "        print(f\"Duplicate '{filename}' moved to duplicate folder.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error moving '{filename}': {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f006807d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(path):\n",
    "    text = \"\"\n",
    "    file_ext= path.split('.')[-1]    \n",
    "    if file_ext == 'docx':\n",
    "        text =docx2txt.process(path)\n",
    "        \n",
    "    elif file_ext == 'pdf':\n",
    "        with open(path, \"rb\") as pdf_file:\n",
    "            pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "            for page_num in range(len(pdf_reader.pages)):\n",
    "                page = pdf_reader.pages[page_num]\n",
    "                text += page.extract_text()\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "470f7734",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_1=extract_text(r\"C:\\Users\\817840\\OneDrive - Cognizant\\2024\\Official\\pythonProject\\Finding duplicates and % match\\Files\\Machine learning - Wikipedia_1page.pdf\")\n",
    "sentences_2=extract_text(r\"C:\\Users\\817840\\OneDrive - Cognizant\\2024\\Official\\pythonProject\\Finding duplicates and % match\\Files\\sample.doc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9ed9f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423c169e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb2046e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6d642df7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffee7abc",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'C:\\\\Users\\\\817840\\\\OneDrive - Cognizant\\\\2024\\\\AI-Engineer-Roadmap-2024\\\\Project POC\\\\Finding duplicates and % match\\\\Files'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13656\\2155355280.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13656\\2155355280.py\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;31m#Check for new documents\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocument_folder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0mfile_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocument_folder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'C:\\\\Users\\\\817840\\\\OneDrive - Cognizant\\\\2024\\\\AI-Engineer-Roadmap-2024\\\\Project POC\\\\Finding duplicates and % match\\\\Files'"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    #load existing hash database or create a new one\n",
    "    if os.path.exists(\"hash_database.txt\"):\n",
    "        with open(\"hash_database.txt\", \"r\") as f:\n",
    "            hash_database = eval(f.read())\n",
    "    else:\n",
    "        hash_database = create_hash_database(document_folder)\n",
    "        with open(\"hash_database.txt\", \"w\") as f:\n",
    "            f.write(str(hash_database))\n",
    "    \n",
    "    #Check for new documents\n",
    "    for filename in os.listdir(document_folder):\n",
    "        file_path = os.path.join(document_folder, filename)\n",
    "        if os.path.isfile(file_path):\n",
    "            #New document: update hash database and potentially move existing duplicates\n",
    "            try:\n",
    "                with open(file_path, \"rb\") as f:\n",
    "                    md5_hash = hashlib.md5(f.read()).hexdigest()\n",
    "                    sha256_hash = hashlib.sha256(f.read()).hexdigest()\n",
    "                hash_database[filename] = (md5_hash, sha256_hash)\n",
    "                \n",
    "                #check for existing duplicates based on MD5 or SHA-256\n",
    "                for existing_filename, existing_hashes in hash_database.items():\n",
    "                    if(existing_filename != filename) and (md5_hash == existing_hashes[0]): #checking for MD5\n",
    "                        #move existing duplicates\n",
    "                        move_duplicate(existing_filename, document_folder, duplicates_folder)\n",
    "                        break #only move one duplicate per new document\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Error: File'{filename}' not found. Skipping...\")\n",
    "\n",
    "    with open(\"hash_database.txt\", \"w\") as f:\n",
    "        f.write(str(hash_database))\n",
    "    print(\"Hash database updated.\")\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "075a31ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('BAAI/bge-large-zh-v1.5')\n",
    "embeddings_1 = model.encode(sentences_1, normalize_embeddings=True)\n",
    "embeddings_2 = model.encode(sentences_2, normalize_embeddings=True)\n",
    "similarity = embeddings_1 @ embeddings_2.T\n",
    "if similarity > 0.80:\n",
    "    print(f\"Similar Document with similarity score of {similarity}\")\n",
    "else:\n",
    "    print(f\"Different Document with similarity score of {similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d58edf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33b9bff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e91304",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4893cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaebf4d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
